## Papers related to generated code detection

Regarding the rising era of LLMs, it is become challenging to develop tools and approaches to detect generated contents. The application is various specially in the education field to attest the autorship of the student's works, in the scientific domain, etc. In this repository, we gather all papers related to generated text detection and specially on code detection.

The following repository will be structured as sub-folders following these topics :

- [🌟 Generated Code Detection](#intro)
- [📜 Papers](#papers)
    - [🏷️ Tagset](#tagset)
    - [🎯 The List](#list)
- [🧰 Resources](#resources)
    - [📊 Datasets](#datasets)
    - [🛠️ Tools](#tools)
- [🚩 Citation](#citation)
- [🎉 Contribution](#contribution)
- [🤝 Acknowledgement](#acknowledgement)


<a id="intro"></a>
## 🌟 Generated Code Detection


<a id="papers"></a>
## 📜 Papers

<a id="tagset"></a>
### 🏷️ Tagset

In this paper list, we tag each paper with one or more labels defined in the table below. These tags serve the purpose of facilitating the related work searching.

| Category | Explanation |
|----------|-------------|
| ![](https://img.shields.io/badge/Analysis-green) | The paper propose a analyse of the existing approach on the generated code detection and the potential features to consider to distinguish human writing from generated code.*. |
| ![](https://img.shields.io/badge/Tools-brown) | The paper propose a tool to detect generated code or to classify a human writing and generated code. This tag will be follow by a category of the proposed approach*. |
| ![](https://img.shields.io/badge/Dataset-blue) | The paper propose a dataset for human_writing / generated code classification. Most of these dataset is constituted by coding problems / contests associated with human writing and generated code (Depending of the model).*|
